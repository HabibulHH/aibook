# Chapter 9: Hands-on ‚Äî Model Download, Run, API Serve, Fine-tune

> "‡¶Ø‡¶•‡ßá‡¶∑‡ßç‡¶ü theory ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶è‡¶ñ‡¶® ‡¶π‡¶æ‡¶§‡ßá ‡¶ï‡¶≤‡¶Æ‡ßá ‚Äî model download ‡¶ï‡¶∞‡ßã, ‡¶ö‡¶æ‡¶≤‡¶æ‡¶ì, API ‡¶¨‡¶æ‡¶®‡¶æ‡¶ì‡•§"

---

## ‡¶è‡¶á Chapter ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ

‡¶è‡¶§‡¶ï‡ßç‡¶∑‡¶£ ‡¶§‡ßÅ‡¶Æ‡¶ø concept ‡¶∂‡¶ø‡¶ñ‡ßá‡¶õ‡ßã ‚Äî tools, data pipeline, transformer, model types, runtime, Hugging Face, production reality‡•§ ‡¶è‡¶á chapter ‡¶è ‡¶ï‡ßã‡¶®‡ßã theory ‡¶®‡¶æ‡¶á‡•§ ‡¶∂‡ßÅ‡¶ß‡ßÅ code‡•§ ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶ï‡¶∞‡¶æ‡•§

‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶æ section ‡¶è ‡¶è‡¶ï‡¶ü‡¶æ task ‡¶Ü‡¶õ‡ßá‡•§ ‡¶§‡ßÅ‡¶Æ‡¶ø follow ‡¶ï‡¶∞‡ßã, run ‡¶ï‡¶∞‡ßã, output ‡¶¶‡ßá‡¶ñ‡ßã‡•§ ‡¶≠‡¶æ‡¶ô‡ßã, ‡¶†‡¶ø‡¶ï ‡¶ï‡¶∞‡ßã‡•§ ‡¶è‡¶≠‡¶æ‡¶¨‡ßá‡¶á ‡¶∂‡ßá‡¶ñ‡¶æ ‡¶π‡¶Ø‡¶º‡•§

---

## Part 1: Setup ‚Äî ‡ß´ ‡¶Æ‡¶ø‡¶®‡¶ø‡¶ü‡ßá Ready ‡¶π‡¶ì

### Python environment

```bash
# Python 3.10+ ‡¶≤‡¶æ‡¶ó‡¶¨‡ßá
python --version

# Virtual environment ‡¶¨‡¶æ‡¶®‡¶æ‡¶ì (recommended)
python -m venv ai-env
source ai-env/bin/activate  # Linux/Mac
# ai-env\Scripts\activate   # Windows

# Core packages install
pip install transformers torch huggingface_hub fastapi uvicorn
```

### Hugging Face CLI (optional ‚Äî public model ‡¶è ‡¶≤‡¶æ‡¶ó‡¶¨‡ßá ‡¶®‡¶æ)

```bash
pip install huggingface_hub
huggingface-cli login
# Token paste ‡¶ï‡¶∞‡ßã: Settings ‚Üí Access Tokens ‚Üí Generate (write)
```

---

## Part 2: ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ Model Run ‚Äî ‡ß© ‡¶≤‡¶æ‡¶á‡¶®‡ßá

### Sentiment Analysis

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
result = classifier("I love building backend systems with NestJS")
print(result)
# [{'label': 'POSITIVE', 'score': 0.9998}]
```

‡¶¨‡ßç‡¶Ø‡¶∏‡•§ ‡ß© ‡¶≤‡¶æ‡¶á‡¶®‡•§ Model automatically download ‡¶π‡¶≤‡ßã, inference ‡¶π‡¶≤‡ßã, result ‡¶™‡ßá‡¶≤‡ßá‡•§

### Text Generation

```python
from transformers import pipeline

generator = pipeline("text-generation", model="gpt2")
result = generator("The future of software engineering is", max_new_tokens=50)
print(result[0]['generated_text'])
```

### Summarization

```python
from transformers import pipeline

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
text = """
Hugging Face is a company that provides tools for building, training and deploying 
machine learning models. Their transformers library has become the standard way to 
work with large language models. The platform hosts hundreds of thousands of 
pre-trained models that developers can use in their applications.
"""
result = summarizer(text, max_length=50, min_length=20)
print(result[0]['summary_text'])
```

### Translation

```python
from transformers import pipeline

translator = pipeline("translation_en_to_fr", model="Helsinki-NLP/opus-mt-en-fr")
result = translator("Hello, how are you?")
print(result[0]['translation_text'])
# Bonjour, comment allez-vous?
```

---

## Part 3: Bigger Model ‚Äî Llama ‡¶ö‡¶æ‡¶≤‡¶æ‡¶ì Locally

### Ollama ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá (‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶∏‡¶π‡¶ú)

```bash
# Ollama install ‡¶ï‡¶∞‡ßã (https://ollama.ai)
curl -fsSL https://ollama.ai/install.sh | sh

# Model download + run
ollama run llama3.2:1b

# Chat ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶π‡¶¨‡ßá terminal ‡¶è
>>> What is a REST API?
```

1B model ‚Äî CPU ‡¶§‡ßá‡¶ì ‡¶ö‡¶≤‡¶¨‡ßá‡•§ GPU ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶®‡¶æ‡¶á‡•§

### Python ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá (Transformers library)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_name = "meta-llama/Llama-3.2-1B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)

prompt = "Explain what a database index is in simple terms:"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---

## Part 4: Embedding ‚Äî Semantic Search ‡¶¨‡¶æ‡¶®‡¶æ‡¶ì

‡¶è‡¶á‡¶ü‡¶æ ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ product ‡¶è ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶Ü‡¶ó‡ßá ‡¶ï‡¶æ‡¶ú‡ßá ‡¶≤‡¶æ‡¶ó‡¶¨‡ßá‡•§

```python
from transformers import AutoTokenizer, AutoModel
import torch

# Embedding model load
tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")

def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

# Documents
docs = [
    "NestJS is a framework for building server-side applications",
    "React is a library for building user interfaces",
    "PostgreSQL is a powerful relational database",
    "Docker containers package applications with dependencies",
]

# Query
query = "How to build a backend API?"
query_emb = get_embedding(query)

# Similarity calculate
import numpy as np

for doc in docs:
    doc_emb = get_embedding(doc)
    similarity = np.dot(query_emb, doc_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(doc_emb))
    print(f"{similarity:.4f} ‚Üí {doc}")

# NestJS wala document er similarity ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶¨‡ßá‡¶∂‡¶ø ‡¶π‡¶¨‡ßá!
```

---

## Part 5: FastAPI ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá Model ‡¶ï‡ßá REST API ‡¶¨‡¶æ‡¶®‡¶æ‡¶ì

‡¶è‡¶á‡¶ü‡¶æ ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ backend experience directly ‡¶ï‡¶æ‡¶ú‡ßá ‡¶≤‡¶æ‡¶ó‡¶¨‡ßá‡•§ Model ‡¶ï‡ßá ‡¶è‡¶ï‡¶ü‡¶æ API endpoint ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá serve ‡¶ï‡¶∞‡ßã‡•§

```python
# app.py
from fastapi import FastAPI
from pydantic import BaseModel
from transformers import pipeline

app = FastAPI()
classifier = pipeline("sentiment-analysis")
generator = pipeline("text-generation", model="gpt2")

class TextInput(BaseModel):
    text: str

class GenerateInput(BaseModel):
    prompt: str
    max_tokens: int = 50

@app.post("/sentiment")
def analyze_sentiment(input: TextInput):
    result = classifier(input.text)
    return {"label": result[0]["label"], "score": result[0]["score"]}

@app.post("/generate")
def generate_text(input: GenerateInput):
    result = generator(input.prompt, max_new_tokens=input.max_tokens)
    return {"generated_text": result[0]["generated_text"]}

@app.get("/health")
def health():
    return {"status": "ok"}
```

Run ‡¶ï‡¶∞‡ßã:

```bash
uvicorn app:app --reload --port 8000
```

Test ‡¶ï‡¶∞‡ßã:

```bash
# Sentiment
curl -X POST http://localhost:8000/sentiment \
  -H "Content-Type: application/json" \
  -d '{"text": "I love coding in TypeScript"}'

# Generate
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "The best database for startups is", "max_tokens": 30}'
```

‡¶¨‡ßç‡¶Ø‡¶∏! ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶è‡¶ñ‡¶® ‡¶è‡¶ï‡¶ü‡¶æ AI-powered REST API ‡¶Ü‡¶õ‡ßá‡•§ NestJS developer ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‚Äî ‡¶è‡¶á‡¶ü‡¶æ ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ comfort zone‡•§ Model serving ‡¶Æ‡¶æ‡¶®‡ßá ‡¶§‡ßã backend engineering ‡¶á‡•§

---

## Part 6: Simple Fine-tune ‚Äî ‡¶®‡¶ø‡¶ú‡ßá‡¶∞ Data ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá

‡¶è‡¶á‡¶ü‡¶æ ‡¶è‡¶ï‡¶ü‡ßÅ advanced ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ doable‡•§ Google Colab (free tier) ‡¶è‡¶ì ‡¶ö‡¶≤‡¶¨‡ßá‡•§

### Data Prepare

```python
# training_data.json
data = [
    {"instruction": "What is NestJS?", "output": "NestJS is a progressive Node.js framework for building efficient server-side applications."},
    {"instruction": "What is PostgreSQL?", "output": "PostgreSQL is an advanced open-source relational database management system."},
    {"instruction": "What is Docker?", "output": "Docker is a platform for building, shipping, and running applications in containers."},
    # ... ‡¶Ü‡¶∞‡¶ì ‡ßß‡ß¶‡ß¶-‡ßß‡ß¶‡ß¶‡ß¶ examples
]
```

### LoRA Fine-tune (QLoRA ‚Äî ‡¶ï‡¶Æ GPU ‡¶§‡ßá ‡¶ö‡¶≤‡ßá)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import Dataset

# Base model load (4-bit quantized)
model_name = "meta-llama/Llama-3.2-1B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,  # QLoRA ‚Äî ‡¶ï‡¶Æ memory ‡¶≤‡¶æ‡¶ó‡¶¨‡ßá
)
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# LoRA config ‚Äî ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶ï‡¶ø‡¶õ‡ßÅ part train ‡¶π‡¶¨‡ßá
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    task_type="CAUSAL_LM",
)
model = get_peft_model(model, lora_config)

# Dataset
dataset = Dataset.from_list([
    {"text": f"### Instruction: {d['instruction']}\n### Response: {d['output']}"}
    for d in data
])

# Train
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    args=TrainingArguments(
        output_dir="./results",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        learning_rate=2e-4,
    ),
)
trainer.train()

# Save + push
model.save_pretrained("./my-fine-tuned-model")
# model.push_to_hub("tomar-username/my-model")  # HF ‡¶§‡ßá upload
```

---

## Part 7: Hugging Face ‡¶è Upload

```python
from huggingface_hub import HfApi

api = HfApi()
api.create_repo("tomar-username/swe-assistant-v1", repo_type="model")
api.upload_folder(
    folder_path="./my-fine-tuned-model",
    repo_id="tomar-username/swe-assistant-v1",
)
print("Model uploaded! üéâ")
```

‡¶è‡¶ñ‡¶® ‡¶Ø‡ßá ‡¶ï‡ßá‡¶â ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ model use ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶¨‡ßá:

```python
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("tomar-username/swe-assistant-v1")
```

---

## Full Workflow ‚Äî ‡¶è‡¶ï‡¶®‡¶ú‡¶∞‡ßá

```
Step 1: pip install transformers torch
Step 2: pipeline() ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá model run ‡¶ï‡¶∞‡ßã (‡ß© ‡¶≤‡¶æ‡¶á‡¶®)
Step 3: Different tasks try ‡¶ï‡¶∞‡ßã (sentiment, generation, embedding)
Step 4: FastAPI ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá REST API ‡¶¨‡¶æ‡¶®‡¶æ‡¶ì
Step 5: ‡¶®‡¶ø‡¶ú‡ßá‡¶∞ data ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá LoRA fine-tune ‡¶ï‡¶∞‡ßã
Step 6: Hugging Face ‡¶è upload ‡¶ï‡¶∞‡ßã
Step 7: Share ‡¶ï‡¶∞‡ßã, iterate ‡¶ï‡¶∞‡ßã
```

---

## PocketSchool Course Flow Suggestion

‡¶è‡¶á chapter ‡¶è‡¶∞ content ‡¶ï‡ßá ‡ß™‡¶ü‡¶æ class ‡¶è ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßã:

**Class 1:** HF setup + pipeline() ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡ß´‡¶ü‡¶æ task run (sentiment, generation, summarization, translation, QA)

**Class 2:** Embedding + semantic search ‡¶¨‡¶æ‡¶®‡¶æ‡¶ì‡•§ pgvector intro‡•§

**Class 3:** FastAPI ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá model serve ‡¶ï‡¶∞‡ßã REST API ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá‡•§ Students ‡¶è‡¶∞ "aha moment" ‚Äî "AI engineering ‡¶Æ‡¶æ‡¶®‡ßá backend engineering ‡¶è‡¶∞ extension!"

**Class 4:** Fine-tune basics‡•§ LoRA ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá small model fine-tune ‡¶ï‡¶∞‡ßã ‡¶®‡¶ø‡¶ú‡ßá‡¶∞ data ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá‡•§

1B model ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá start ‡¶ï‡¶∞‡ßã ‡¶∏‡¶¨‡¶∏‡¶Æ‡¶Ø‡¶º‡•§ GPU ‡¶õ‡¶æ‡¶°‡¶º‡¶æ‡¶á CPU ‡¶§‡ßá ‡¶ö‡¶≤‡¶¨‡ßá‡•§ Students ‡¶è‡¶∞ laptop ‡¶è‡¶ì run ‡¶ï‡¶∞‡¶¨‡ßá‡•§ ‡¶™‡¶∞‡ßá bigger model ‡¶¶‡ßá‡¶ñ‡¶æ‡¶ì‡•§

---

## ‡¶è‡¶á Chapter ‡¶è ‡¶ï‡ßÄ ‡¶∂‡¶ø‡¶ñ‡¶≤‡ßá?

‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶π‡¶æ‡¶§‡ßá ‡¶ï‡¶≤‡¶Æ‡ßá ‡¶ï‡¶∞‡¶≤‡ßá ‚Äî model download, run, embedding, API serve, fine-tune, upload‡•§ ‡¶è‡¶á‡¶ü‡¶æ‡¶á ‡¶õ‡¶ø‡¶≤‡ßã ‡¶™‡ßÅ‡¶∞‡ßã ‡¶¨‡¶á‡¶Ø‡¶º‡ßá‡¶∞ ‡¶≤‡¶ï‡ßç‡¶∑‡ßç‡¶Ø ‚Äî theory ‡¶¨‡ßã‡¶ù‡ßã, ‡¶§‡¶æ‡¶∞‡¶™‡¶∞ ‡¶ï‡¶∞‡ßã‡•§

‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ AI journey ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶π‡¶Ø‡¶º‡ßá ‡¶ó‡ßá‡¶õ‡ßá‡•§ ‡¶è‡¶ñ‡¶® iterate ‡¶ï‡¶∞‡ßã, build ‡¶ï‡¶∞‡ßã, break ‡¶ï‡¶∞‡ßã, fix ‡¶ï‡¶∞‡ßã‡•§ Happy building! üöÄ
